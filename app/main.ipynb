{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed9b1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "from mtcnn import MTCNN\n",
    "from mtcnn.utils.images import load_image, load_images_batch\n",
    "from mtcnn.utils.plotting import plot\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6b2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class App:\n",
    "    class NoFaceDetectedError(Exception):\n",
    "        \"\"\"Raised when no face is detected in an image.\"\"\"\n",
    "        def __init__(self, message=\"No face detected in the provided image.\"):\n",
    "            super().__init__(message)\n",
    "\n",
    "    class VideoStream:\n",
    "        \"\"\"Threaded camera capture to avoid blocking\"\"\"\n",
    "        def __init__(self, src=0):\n",
    "            self.cap = cv2.VideoCapture(src)\n",
    "            if not self.cap.isOpened():\n",
    "                raise RuntimeError(\"Failed to open camera or video file.\")\n",
    "            self.q = queue.Queue(maxsize=1)\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(target=self.update, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "        def update(self):\n",
    "            while self.running:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.running = False\n",
    "                    break\n",
    "                if not self.q.full():\n",
    "                    self.q.put(frame)\n",
    "\n",
    "        def read(self):\n",
    "            if not self.q.empty():\n",
    "                return self.q.get()\n",
    "            return None\n",
    "\n",
    "        def release(self):\n",
    "            self.running = False\n",
    "            self.thread.join()\n",
    "            self.cap.release()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.KNOWN_FACES_PATH = \"../known_faces\"\n",
    "        self.UNKNOWN_FACES_PATH = \"../unknown_faces\"\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.mtcnn = MTCNN(device=\"GPU:0\")\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        self.chroma_client = chromadb.PersistentClient(\"../chromadb\")\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"faces\",\n",
    "            embedding_function=None,\n",
    "            configuration={\n",
    "                \"hnsw\": {\n",
    "                    \"space\": \"l2\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        self.process_known_images()\n",
    "\n",
    "    def select_random_known_img_path(self, KNOWN_FACES_PATH: str, k: int):\n",
    "        res = []\n",
    "        for i in range(k):\n",
    "            folder_name = random.choice(os.listdir(KNOWN_FACES_PATH))\n",
    "            folder_path = os.path.join(KNOWN_FACES_PATH, folder_name)\n",
    "            file_name = random.choice(os.listdir(folder_path))\n",
    "            res.append(os.path.join(folder_path, file_name))\n",
    "        return res\n",
    "\n",
    "    def detect_significant_face(self, img):\n",
    "        faces = self.mtcnn.detect_faces(img, box_format=\"xywh\")\n",
    "        if not faces:\n",
    "            raise App.NoFaceDetectedError\n",
    "        significant_face = max(faces, key=lambda x: x['confidence'])\n",
    "        return significant_face\n",
    "\n",
    "    def crop_img(self, img, x, y, w, h):\n",
    "        x1, y1 = x, y\n",
    "        x2, y2 = x + w, y + h\n",
    "\n",
    "        cropped_img = img[y1:y2, x1:x2]\n",
    "        return cropped_img\n",
    "\n",
    "    def img_to_tensor(self, img):\n",
    "        resized_img = cv2.resize(img, (160, 160))\n",
    "        tensor = torch.tensor(resized_img).permute(2, 0, 1).float()  # CxHxW\n",
    "        return tensor.unsqueeze(0).to(self.device)\n",
    "\n",
    "    def normalize_tensor(self, tensor):\n",
    "        return (tensor - 127.5) / 128.0\n",
    "\n",
    "    def embed_image(self, img):\n",
    "        return self.facenet(img).detach().cpu()\n",
    "\n",
    "    def process_known_images(self):\n",
    "        for name in tqdm(os.listdir(self.KNOWN_FACES_PATH)):\n",
    "            for image_name in os.listdir(os.path.join(self.KNOWN_FACES_PATH, name)):\n",
    "                img_path = os.path.join(self.KNOWN_FACES_PATH, name, image_name)\n",
    "                existing_record = self.collection.get(where={\"img_path\": img_path})\n",
    "                if existing_record[\"ids\"]:\n",
    "                    continue\n",
    "                try:\n",
    "                    img = load_image(img_path)\n",
    "                    detection_result = self.detect_significant_face(img)\n",
    "                    cropped_img = self.crop_img(img, *detection_result['box'])\n",
    "                    face_tensor = self.img_to_tensor(cropped_img)\n",
    "                    normalized_tensor = self.normalize_tensor(face_tensor)\n",
    "                    face_embedding = self.embed_image(normalized_tensor)\n",
    "                    self.collection.add(\n",
    "                        ids=image_name,\n",
    "                        embeddings=face_embedding.tolist(),\n",
    "                        metadatas={\n",
    "                            \"name\": name,\n",
    "                            \"img_path\": img_path\n",
    "                        }\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't process file {img_path}\\nError: {e}\")\n",
    "                    continue\n",
    "\n",
    "    def process_unknown_images(self, results_path, tolerance: float = 0.5):\n",
    "        data = []\n",
    "        for img_name in tqdm(os.listdir(self.UNKNOWN_FACES_PATH)):\n",
    "            img_path = os.path.join(self.UNKNOWN_FACES_PATH, img_name)\n",
    "            try:\n",
    "                results = self.search_image(img_path, 1)\n",
    "                predicted_name = \"UNKNOWN\"\n",
    "                predicted_image_path = None\n",
    "                distance = None\n",
    "                if results['distances'][0][0] <= tolerance:\n",
    "                    predicted_name = results['metadatas'][0][0]['name']\n",
    "                    predicted_image_path = results['metadatas'][0][0]['img_path']\n",
    "                    distance = results['distances'][0][0]\n",
    "\n",
    "                actual_name = \"_\".join(img_name.split('_')[:-1])\n",
    "                is_correct = actual_name == predicted_name\n",
    "                row = {\n",
    "                    \"actual_name\": actual_name,\n",
    "                    \"predicted_name\": predicted_name,\n",
    "                    \"is_correct\": is_correct,\n",
    "                    \"predicted_image_path\": predicted_image_path,\n",
    "                    \"known_image_path\": img_path,\n",
    "                    \"distance\": distance\n",
    "                }\n",
    "                data.append(row)\n",
    "\n",
    "            except App.NoFaceDetectedError:\n",
    "                print(f\"No face detected in image: {img_path}\")\n",
    "                continue\n",
    "        pd.DataFrame(data).to_csv(results_path, index=False)\n",
    "    \n",
    "    def search_image(self, path, n_results):\n",
    "        img = load_image(path)\n",
    "        return self.search_frame(img, n_results)\n",
    "\n",
    "    def search_frame(self, frame, n_results):\n",
    "        detection_result = self.detect_significant_face(frame)\n",
    "        cropped_img = self.crop_img(frame, *detection_result['box'])\n",
    "        face_tensor = self.img_to_tensor(cropped_img)\n",
    "        normalized_tensor = self.normalize_tensor(face_tensor)\n",
    "        face_embedding = self.embed_image(normalized_tensor)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=face_embedding.tolist(),\n",
    "            n_results=n_results,\n",
    "            include=[\"embeddings\", \"metadatas\", \"distances\", \"documents\"]\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def video_recognize(self, tolerance: float = 0.5, frame_step: int = 5):\n",
    "        \"\"\"\n",
    "        Recognize faces from video stream, processing every `frame_step`-th frame.\n",
    "\n",
    "        Parameters:\n",
    "            tolerance (float): Distance threshold for a match.\n",
    "            frame_step (int): Process every n-th frame. Example: frame_step=5 -> process every 5th frame.\n",
    "        \"\"\"\n",
    "        # Open default camera (0). You can replace 0 with a video file path.\n",
    "        stream = self.VideoStream(0)\n",
    "        print(\"Press 'q' or 'Esc' to quit.\")\n",
    "\n",
    "        # Initialize timer for FPS calculation\n",
    "        prev_time = time.time()\n",
    "        fps = 0.0\n",
    "\n",
    "        # Frame counter\n",
    "        frame_count = 0\n",
    "\n",
    "        # Cache last detection\n",
    "        last_detection = None\n",
    "        last_name = \"UNKNOWN\"\n",
    "        last_matched_image = None\n",
    "\n",
    "        while stream.running:\n",
    "            frame = stream.read()\n",
    "            if frame is None:\n",
    "                continue\n",
    "\n",
    "            frame_count += 1\n",
    "            # ✅ Process only every n-th frame\n",
    "            if frame_count % frame_step == 0:\n",
    "                try:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    detection_result = self.detect_significant_face(frame_rgb)\n",
    "                    x, y, w, h = detection_result['box']\n",
    "\n",
    "                    cropped_img = self.crop_img(frame_rgb, x, y, w, h)\n",
    "                    face_tensor = self.img_to_tensor(cropped_img)\n",
    "                    normalized_tensor = self.normalize_tensor(face_tensor)\n",
    "                    face_embedding = self.embed_image(normalized_tensor)\n",
    "\n",
    "                    results = self.collection.query(\n",
    "                        query_embeddings=face_embedding.tolist(),\n",
    "                        n_results=1,\n",
    "                        include=[\"embeddings\", \"metadatas\", \"distances\", \"documents\"]\n",
    "                    )\n",
    "\n",
    "                    predicted_name = \"UNKNOWN\"\n",
    "                    matched_image = None\n",
    "                    if results['distances'][0][0] <= tolerance:\n",
    "                        predicted_name = results['metadatas'][0][0]['name']\n",
    "                        matched_image_path = results['metadatas'][0][0]['img_path']\n",
    "                        matched_image = cv2.imread(matched_image_path)\n",
    "                        \n",
    "                        if matched_image is not None:\n",
    "                            frame_h = frame.shape[0]\n",
    "                            aspect_ratio = matched_image.shape[1] / matched_image.shape[0]\n",
    "                            new_w = int(frame_h * aspect_ratio)\n",
    "                            matched_image = cv2.resize(matched_image, (new_w, frame_h))\n",
    "\n",
    "                    # ✅ Cache the detection\n",
    "                    last_detection = (x, y, w, h)\n",
    "                    last_name = predicted_name\n",
    "                    last_matched_image = matched_image\n",
    "\n",
    "                except self.NoFaceDetectedError:\n",
    "                    pass\n",
    "\n",
    "            # ✅ Always redraw from cache\n",
    "            if last_detection:\n",
    "                x, y, w, h = last_detection\n",
    "                top_left, bottom_right = (x, y), (x+w, y+h)\n",
    "                cv2.rectangle(frame, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "                # Label\n",
    "                label_y = y + h + 20 if (y + h + 20) < frame.shape[0] else y - 10\n",
    "                cv2.putText(frame, last_name, (x, label_y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "                # Matched image (if available)\n",
    "                if last_matched_image is not None:\n",
    "                    frame = cv2.hconcat([frame, last_matched_image])\n",
    "\n",
    "            # Calculate FPS\n",
    "            curr_time = time.time()\n",
    "            elapsed = curr_time - prev_time\n",
    "            if elapsed > 0:\n",
    "                fps = 0.9 * fps + 0.1 * (1 / elapsed)\n",
    "            prev_time = curr_time\n",
    "\n",
    "            # Display FPS on frame\n",
    "            cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "            # Show the frame\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "\n",
    "            # Wait for 1 ms and check for key press\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q') or key == 27:  # 27 is Esc\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "\n",
    "            # Cap the FPS to prevent CPU from running 100%\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        # Release resources\n",
    "        stream.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b054204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1084/1681 [00:03<00:02, 199.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't process file ../known_faces\\Marilyn_Monroe\\Marilyn_Monroe_0001.jpg\n",
      "Error: No face detected in the provided image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1681/1681 [00:06<00:00, 274.10it/s]\n"
     ]
    }
   ],
   "source": [
    "app = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87d979e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['Abdoulaye_Wade_0001.jpg']],\n",
       " 'embeddings': [array([[-0.01500503, -0.00814223, -0.08948263,  0.08022343, -0.02133966,\n",
       "           0.04164231,  0.02203878, -0.0089441 , -0.08365081, -0.04914057,\n",
       "          -0.00914059, -0.03725747, -0.02519825, -0.1073973 , -0.04476332,\n",
       "          -0.06416232,  0.02533201,  0.02871547,  0.01664347, -0.09567705,\n",
       "          -0.03124513,  0.03051565, -0.04863874,  0.08989248, -0.00244354,\n",
       "           0.05888009,  0.08617198,  0.02023518,  0.05148148,  0.07205155,\n",
       "           0.0379464 , -0.01517769, -0.00086331, -0.0138928 , -0.05126122,\n",
       "           0.03332475, -0.03779221,  0.02241295,  0.03552938,  0.01191207,\n",
       "          -0.02577062, -0.05721798,  0.05069862, -0.08422723,  0.03962384,\n",
       "          -0.00718057, -0.09832343, -0.07272041,  0.08498655,  0.01572044,\n",
       "           0.01590642,  0.0754925 ,  0.04452571,  0.05517029, -0.01918009,\n",
       "          -0.01369224,  0.05149269, -0.00765376,  0.06864413,  0.00968034,\n",
       "           0.05867082, -0.01660311,  0.06783019, -0.01929546,  0.05054759,\n",
       "           0.04095927, -0.0076225 , -0.00016908, -0.05771721,  0.06350493,\n",
       "          -0.05503772,  0.02429295,  0.02093719,  0.03739101,  0.0134186 ,\n",
       "           0.0124307 , -0.08253198, -0.04968976, -0.04134327,  0.01817229,\n",
       "          -0.0224822 ,  0.01576991,  0.05583118,  0.03352781,  0.04497578,\n",
       "          -0.03007681, -0.02001134, -0.0334059 ,  0.09365281, -0.06812607,\n",
       "           0.02440461, -0.00884519, -0.01453401,  0.06121156,  0.04222891,\n",
       "           0.01274664,  0.01465208, -0.05004735,  0.00067012,  0.05284507,\n",
       "          -0.03186007, -0.10548293,  0.01389077, -0.04073055, -0.05369457,\n",
       "          -0.0100725 , -0.04582372, -0.02539637,  0.02459989, -0.01469881,\n",
       "           0.01980373,  0.08224996, -0.06411666, -0.03115809,  0.03404603,\n",
       "          -0.0567298 , -0.08814137, -0.05970965, -0.04355309,  0.02739709,\n",
       "          -0.03692435,  0.03400062, -0.0689566 , -0.00059112, -0.04111601,\n",
       "          -0.03627678, -0.03356845,  0.0330293 ,  0.05174417,  0.05034721,\n",
       "          -0.01412981, -0.0421993 ,  0.03970172,  0.0694501 , -0.08131288,\n",
       "           0.00049421, -0.0018136 , -0.00276493, -0.01915968,  0.04689246,\n",
       "           0.00487834,  0.049102  ,  0.03356009,  0.03099762, -0.03585053,\n",
       "          -0.02417338, -0.00287369, -0.04119617,  0.01526542,  0.04424698,\n",
       "          -0.0126708 ,  0.07303367,  0.0298564 ,  0.02015781,  0.05694907,\n",
       "          -0.00218503, -0.00542067, -0.02140058,  0.07547959, -0.00410361,\n",
       "           0.03901511, -0.0077006 ,  0.03127249,  0.02645491, -0.04023478,\n",
       "           0.01527173,  0.01826756, -0.00378519,  0.03321448,  0.01984569,\n",
       "           0.00747384,  0.03613245,  0.0183188 ,  0.02869137, -0.036753  ,\n",
       "           0.08678161,  0.00954202, -0.00130096,  0.02417608, -0.06663655,\n",
       "          -0.02301549, -0.08874951, -0.01095371, -0.006303  , -0.04362357,\n",
       "           0.01232477, -0.00213778,  0.00119135, -0.01454995,  0.0311631 ,\n",
       "          -0.04796574, -0.03016832, -0.01867641, -0.06707796, -0.0680003 ,\n",
       "          -0.06527503,  0.03639276, -0.00302138,  0.01122259, -0.00750177,\n",
       "          -0.07006949,  0.05751241, -0.03534657,  0.01125425,  0.05313836,\n",
       "          -0.02786496,  0.00999429, -0.089645  , -0.03841148, -0.01886545,\n",
       "           0.057213  ,  0.00256152,  0.00887524,  0.08202963, -0.00110135,\n",
       "           0.03088225, -0.05364744,  0.02738246, -0.02217025, -0.02825768,\n",
       "           0.01911759, -0.0854554 , -0.05574596,  0.03018323,  0.00104802,\n",
       "          -0.12359063, -0.00405858,  0.00444631,  0.03217096, -0.05401553,\n",
       "           0.0550092 ,  0.06536245,  0.01133333, -0.03810493, -0.02064116,\n",
       "          -0.05839065,  0.00180877,  0.09943751, -0.05770033, -0.09771626,\n",
       "           0.02751953, -0.0229815 , -0.05671139,  0.00339588,  0.0366574 ,\n",
       "           0.01435623,  0.04236763,  0.00535556, -0.02598406, -0.01907953,\n",
       "          -0.05167501, -0.00759675,  0.00569347,  0.04334851,  0.03302987,\n",
       "          -0.05590609,  0.0185365 , -0.04972762,  0.07278676, -0.01001154,\n",
       "           0.06902588,  0.00987816, -0.0226168 , -0.0529663 ,  0.0170083 ,\n",
       "          -0.05298298, -0.02862222,  0.00916836, -0.0848406 , -0.07574613,\n",
       "           0.06306891,  0.02368253,  0.00986929,  0.02830648,  0.08835352,\n",
       "           0.05115694,  0.0215868 ,  0.01629957, -0.0241184 ,  0.01998402,\n",
       "           0.04326345, -0.05063907, -0.05550487, -0.02251709, -0.07155406,\n",
       "           0.00381285, -0.03729005,  0.0377151 , -0.01011526,  0.01194621,\n",
       "          -0.03539787, -0.05334156, -0.02039087, -0.04519836,  0.01387962,\n",
       "          -0.02135596,  0.00599318,  0.03317493,  0.0441581 ,  0.03528646,\n",
       "           0.10645386, -0.03652253,  0.08270549, -0.05710195, -0.02599707,\n",
       "          -0.01915189,  0.03503317,  0.03105831,  0.04296045,  0.00850096,\n",
       "           0.05124246, -0.03583069,  0.06209122, -0.00635459,  0.00344698,\n",
       "           0.04141555,  0.0317315 , -0.02700349,  0.00600308,  0.01952828,\n",
       "           0.06940023, -0.03869935,  0.03377279, -0.00562984,  0.02677158,\n",
       "          -0.05518426,  0.01518744, -0.08062456,  0.01649749, -0.00686626,\n",
       "           0.05457643, -0.02446149,  0.03081167,  0.07113842, -0.01303489,\n",
       "           0.00907341, -0.10487946, -0.02121673,  0.03649445,  0.01296922,\n",
       "           0.02990166, -0.03262835,  0.05115033, -0.06654252,  0.02482696,\n",
       "          -0.07902905, -0.01983878, -0.03635804,  0.06913711, -0.01258282,\n",
       "          -0.07713471, -0.0123931 , -0.06604336, -0.04181118, -0.02001146,\n",
       "           0.06597738,  0.02758609,  0.00406961, -0.00645971, -0.00616937,\n",
       "          -0.06447721, -0.00731584,  0.00947119,  0.02162902,  0.06655014,\n",
       "          -0.02253644,  0.06413989,  0.02951703, -0.04962326, -0.03044645,\n",
       "           0.09453604,  0.00019496, -0.00661661, -0.01013785, -0.01418911,\n",
       "          -0.0337174 ,  0.01590846,  0.01363035, -0.07355931, -0.11035135,\n",
       "          -0.00133607,  0.03842063, -0.03075967, -0.01490846, -0.03391365,\n",
       "           0.00751671, -0.03070474, -0.04853651,  0.03760085,  0.05078055,\n",
       "           0.02920485,  0.02128107, -0.00177646, -0.0423028 , -0.05192602,\n",
       "           0.0276709 ,  0.01659058,  0.0528871 , -0.04158403,  0.00669044,\n",
       "           0.00785702,  0.01778597, -0.05885162,  0.06772612,  0.00064307,\n",
       "           0.01673301, -0.00969744,  0.02937297, -0.06216535,  0.01556245,\n",
       "           0.02026871,  0.02914453,  0.00223524,  0.00155245,  0.05871362,\n",
       "          -0.04428095, -0.00651489,  0.02835138, -0.08262257,  0.06791219,\n",
       "          -0.01841892,  0.00273492,  0.02705098, -0.03631971, -0.05491307,\n",
       "           0.0217326 ,  0.05919811,  0.08376116,  0.06674001,  0.03036782,\n",
       "          -0.021887  ,  0.02577   , -0.05112944, -0.0191821 , -0.02131743,\n",
       "          -0.0466876 , -0.03457181, -0.01546282,  0.09377058,  0.03703303,\n",
       "          -0.01072824,  0.05740361, -0.07601683, -0.01393666, -0.05111688,\n",
       "           0.02762795, -0.04897188,  0.09265913, -0.02552572, -0.04231062,\n",
       "           0.01682108, -0.02834582, -0.00615495, -0.04000897, -0.03795241,\n",
       "           0.0525225 , -0.04129351,  0.00403079,  0.01056121,  0.05949578,\n",
       "          -0.05640278,  0.04255871,  0.01596161,  0.07869934,  0.01883001,\n",
       "          -0.01467335,  0.06159194, -0.05389421, -0.02339699, -0.07903416,\n",
       "          -0.00787932, -0.03932261,  0.01349794,  0.00676972, -0.01420619,\n",
       "           0.03375657,  0.00632284, -0.03981366,  0.03254864, -0.07557804,\n",
       "           0.04814097, -0.03903621,  0.02603493, -0.02920201, -0.02675619,\n",
       "          -0.0373789 , -0.10510353,  0.05553409, -0.11319349,  0.01277725,\n",
       "           0.07410831,  0.04093211,  0.04802246, -0.0261353 ,  0.05632238,\n",
       "           0.02092948, -0.01378986,  0.01040885,  0.04179992,  0.03082829,\n",
       "           0.05197696,  0.02847694,  0.04655183,  0.02696668,  0.01200645,\n",
       "           0.04571878,  0.0527758 ,  0.00961581,  0.026533  , -0.00365098,\n",
       "           0.04494718,  0.00715989]])],\n",
       " 'documents': [[None]],\n",
       " 'uris': None,\n",
       " 'included': ['embeddings', 'metadatas', 'distances', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'name': 'Abdoulaye_Wade',\n",
       "    'img_path': '../known_faces\\\\Abdoulaye_Wade\\\\Abdoulaye_Wade_0001.jpg'}]],\n",
       " 'distances': [[0.4661758244037628]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.search_image(\"../unknown_faces/Abdoulaye_Wade_0004.jpg\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4112fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, tol in enumerate(np.arange(0.5, 1.0, 0.05), start=1):\n",
    "#    tol = np.round(tol, 2)\n",
    "#    app.process_unknown_images(f\"../analysis/results{str(index)}.csv\", tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f8bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' or 'Esc' to quit.\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "app.video_recognize(tolerance=0.55, frame_step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95985d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
